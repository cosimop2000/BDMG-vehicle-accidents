{
    "Input": [
        {
            "method": "load_dataset",
            "input_pandas": {
                "sep": ","
            },

            "input_vaex": {
            },
            "input_polars":{
                "sep": ","
            },
            "input_datatable":{
                "sep": ","
            },
            "input_spark":{
                "sep": ",",
                "format": "csv"
            },
            "input_dask":{
                "sep": ","
            },
            "input_pyspark_pandas":{
                "sep": ",",
                "format": "csv"
            },
            "input_modin_ray":{

            }
        },
        {
            "method": "force_execution",
            "input": {}
        }
    ],

    "EDA": [
        {
            "method": "get_columns",
            "input":{}
        },
        
        {
            "method": "get_stats",
            "input": {}
        },
        {
            "method": "perc_null_values",
            "input":{}
        },
        {
            "method":"query",
            "input_pandas":{
                "query":"(Start_Lat == End_Lat) & (Start_Lng == End_Lng)"
            },
            "input_vaex":{
                "query":"(Start_Lat == End_Lat) & (Start_Lng == End_Lng)"
            },
            "input_spark":{
                "query":"(fn.col('Start_Lat') == fn.col('End_Lat')) & (fn.col('Start_Lng') == fn.col('End_Lng'))"
            },
            "input_polars": {
                "query": "(self.df_['Start_Lat'] == self.df_['End_Lat']) & (self.df_['Start_Lng'] == self.df_['End_Lng'])"
            },
            "input_datatable":{
                "query":"(f.Start_Lat == f.End_Lat) & (f.Start_Lng == f.End_Lng)"
            },
            "input_pyspark_pandas":{
                "query":"(Start_Lat == End_Lat) and (Start_Lng == End_Lng)"
            },
            "input_dask":{
                "query": "(Start_Lat == End_Lat) & (Start_Lng == End_Lng)"
            }
        },
        {
            "method":"check_missing_values",
            "input_pandas":{
                "col1": "End_Lat",
                "col2": "End_Lng"
            },
            "input_vaex":{
                "col1": "End_Lat",
                "col2": "End_Lng"
            },
            "input_spark":{
                "col1": "End_Lat",
                "col2": "End_Lng"
            },
            "input_polars": {
                "col1": "End_Lat",
                "col2": "End_Lng"
            },
            "input_datatable":{
                "col1": "End_Lat",
                "col2": "End_Lng"
            },
            "input_pyspark_pandas":{
                "col1": "End_Lat",
                "col2": "End_Lng"
            },
            "input_dask": {
                "col1": "End_Lat",
                "col2": "End_Lng"
            }
        },
        {
            "method":"is_unique",
            "input_pandas":{
                "column":"ID"
            },
            "input_vaex":{
                "column":"ID"
            },
            "input_spark":{
                "column":"ID"
            },
            "input_polars": {
                "column": "ID"
            },
            "input_datatable":{
                "column":"ID"
            },
            "input_pyspark_pandas":{
                "column":"ID"
            },
            "input_dask":{
                "column":"ID"
            }
        },
        {
            "method":"is_unique",
            "input_pandas":{
                "column":"Description"
            },
            "input_vaex":{
                "column":"Description"
            },
            "input_spark":{
                "column":"Description"
            },
            "input_polars": {
                "column": "Description"
            },
            "input_datatable":{
                "column":"Description"
            },
            "input_pyspark_pandas":{
                "column":"Description"
            },
            "input_dask":{
                "column":"Description"
            }
        },
        {
            "method":"is_unique",
            "input_pandas":{
                "column":"Wind_Direction"
            },
            "input_vaex":{
                "column":"Wind_Direction"
            },
            "input_spark":{
                "column":"Wind_Direction"
            },
            "input_polars": {
                "column": "Wind_Direction"
            },
            "input_datatable": {
                "column": "Wind_Direction"
            },
            "input_pyspark_pandas":{
                "column":"Wind_Direction"
            },
            "input_dask":{
                "column":"Wind_Direction"
            }
        },
        {
            "method":"locate_null_values",
            "input_pandas":{
                "column":"Airport_Code"
            },
            "input_vaex":{
                "column":"Airport_Code"
            },
            "input_spark":{
                "column":"Airport_Code"
            },
            "input_polars": {
                "column": "Airport_Code"
            },
            "input_datatable": {
                "column": "Airport_Code"
            },
            "input_pyspark_pandas": {
                "column": "Airport_Code"
            },
            "input_dask": {
                "column": "Airport_Code"
            }
        },
        {
            "method":"query",
            "input_pandas":{
                "query":"`Precipitation(in)` == 0"
            },
            "input_vaex":{
                "query":"`Precipitation(in)` == 0"
            },
            "input_spark":{
                "query":"fn.col('Precipitation(in)') == 0"
            },
            "input_polars": {
                "query": "pl.col('Precipitation(in)') == 0"
            },
            "input_datatable":{
                "query":"f['Precipitation(in)'] == 0"
            },
            "input_pyspark_pandas": {
                "query": "`Precipitation(in)` == 0"
            }
        },
        {
            "method":"look_for_cases",
            "input_pandas":{
                "col1":"Humidity(%)",
                "col2":"Precipitation(in)",
                "col3":"Humidity(%)",
                "col4":"Precipitation(in)"
            },
            "input_vaex":{
                "col1":"Humidity(%)",
                "col2":"Precipitation(in)",
                "col3":"Humidity(%)",
                "col4":"Precipitation(in)"     
            },
            "input_spark":{
                "col1":"Humidity(%)",
                "col2":"Precipitation(in)",
                "col3":"Humidity(%)",
                "col4":"Precipitation(in)"     
            },
            "input_polars": {
                "col1": "Humidity(%)",
                "col2": "Precipitation(in)"
            },
            "input_datatable":{
                "col1":"Humidity(%)",
                "col2":"Precipitation(in)",
                "col3":"Humidity(%)",
                "col4":"Precipitation(in)"     
            },
            "input_pyspark_pandas": {
                "col1": "Humidity(%)",
                "col2": "Precipitation(in)"
            }
        },
        {
            "method":"look_for_cases",
            "input_pandas":{
                "col1":"Wind_Speed(mph)",
                "col2":"Wind_Direction",
                "col3":"Wind_Speed(mph)",
                "col4":"Wind_Direction"
            },
            "input_vaex":{
                "col1":"Wind_Speed(mph)",
                "col2":"Wind_Direction",
                "col3":"Wind_Speed(mph)",
                "col4":"Wind_Direction"
            },
            "input_spark":{
                "col1":"Wind_Speed(mph)",
                "col2":"Wind_Direction",
                "col3":"Wind_Speed(mph)",
                "col4":"Wind_Direction"
            },
            "input_polars": {
                "col1": "Wind_Speed(mph)",
                "col2": "Wind_Direction"
            },
            "input_datatable":{
                "col1":"Wind_Speed(mph)",
                "col2":"Wind_Direction",
                "col3":"Wind_Speed(mph)",
                "col4":"Wind_Direction"
            },
            "input_pyspark_pandas": {
                "col1": "Wind_Speed(mph)",
                "col2": "Wind_Direction"
            }
        },
        {
            "method":"look_for_cases",
            "input_pandas":{
                "col1":"Wind_Speed(mph)",
                "col2":"Wind_Chill(F)",
                "col3":"Wind_Speed(mph)",
                "col4":"Wind_Chill(F)"
            },
            "input_vaex":{
                "col1":"Wind_Speed(mph)",
                "col2":"Wind_Chill(F)",
                "col3":"Wind_Speed(mph)",
                "col4":"Wind_Chill(F)"    
            },
            "input_spark":{
                "col1":"Wind_Speed(mph)",
                "col2":"Wind_Chill(F)",
                "col3":"Wind_Speed(mph)",
                "col4":"Wind_Chill(F)"    
            },
            "input_polars": {
                "col1": "Wind_Speed(mph)",
                "col2": "Wind_Chill(F)"
            },
            "input_datatable":{
                "col1":"Wind_Speed(mph)",
                "col2":"Wind_Chill(F)",
                "col3":"Wind_Speed(mph)",
                "col4":"Wind_Chill(F)"
            },
            "input_pyspark_pandas": {
                "col1": "Wind_Speed(mph)",
                "col2": "Wind_Chill(F)"
            }
        },
        {
            "method":"sample_rows",
            "input_pandas":{
                "frac": true,
                "num": 1
            },
            "input_vaex":{
                "frac": true,
                "num": 1    
            },
            "input_spark":{
                "frac": true,
                "num": 1    
            },
            "input_polars": {
                "frac": true,
                "num": 1
            },
            "input_datatable":{
                "frac": true,
                "num": 1
            },
            "input_pyspark_pandas":{
                "frac": true,
                "num": 1
            }
        },
        {
            "method":"sort",
            "input_pandas":{
                "columns":["Weather_Condition"]
            },
            "input_vaex":{
                "columns":["Weather_Condition"]
            },
            "input_spark":{
                "columns":["Weather_Condition"]
            },
            "input_polars": {
                "columns": ["Weather_Condition"]
            },
            "input_datatable":{
                "columns":["Weather_Condition"]
            },
            "input_pyspark_pandas":{
                "columns":["Weather_Condition"]
            }
        },
        {
            "method":"plot_geo",
            "input_pandas":{
                "frame": "gdfs_sample",
                "i": "Severity",
                "req_compile":[
                    "frame"
                ],
                "extra_commands":[
                    "import geopandas",
                    "import geoplot as gplt",
                    "import geoplot.crs as gcrs",
                    "import matplotlib.pyplot as plt",
                    "import pandas as pd",
                    "accident_data = pd.read_csv(r'datasets/test/sample.csv')",
                    "severity_locations = accident_data[['Temperature(F)','Wind_Chill(F)','Severity','Start_Lng','Start_Lat']]",
                    "gdf_severity = geopandas.GeoDataFrame(severity_locations, geometry=geopandas.points_from_xy(severity_locations.Start_Lng, severity_locations.Start_Lat))",
                    "gdfs_sample = gdf_severity.sample(int(len(gdf_severity)/10))",
                    "print(gdfs_sample)"

                ]
            },
            "input_vaex":{
                "frame": "gdfs_sample",
                "i": 2,
                "req_compile":[
                    "frame"
                ],
                "extra_commands":[
                    "import geopandas",
                    "import geoplot as gplt",
                    "import geoplot.crs as gcrs",
                    "import matplotlib.pyplot as plt",
                    "import pandas as pd",
                    "import numpy as np",
                    "import vaex as vx",
                    "accident_data = vx.read_csv(r'datasets/test/sample.csv')",
                    "severity_locations = accident_data[['Temperature(F)','Wind_Chill(F)','Severity','Start_Lng','Start_Lat']]",
                    "start_lng = accident_data['Start_Lng'].to_numpy()",
                    "start_lat = accident_data['Start_Lat'].to_numpy()",
                    "gdf_severity = geopandas.GeoDataFrame(severity_locations, geometry=geopandas.points_from_xy(start_lng, start_lat))",
                    "gdfs_sample = gdf_severity.sample(int(len(gdf_severity)/10))",
                    "print(gdfs_sample)"

                ]
            },
            "input_spark":{
                "frame": "gdfs_sample",
                "i":  "Severity",
                "req_compile":[
                    "frame"
                ],
                "extra_commands":[
                    "import geopandas",
                    "import geoplot as gplt",
                    "import geoplot.crs as gcrs",
                    "import pandas as pd",
                    "import numpy as np",
                    "from pyspark.sql import DataFrame, SparkSession",
                    "from pyspark.sql.functions import col, expr, lit",
                    "import matplotlib.pyplot as plt",
                    "import pyspark.sql.functions as fn",
                    "accident_data = pd.read_csv(r'datasets/US_Accidents_March23/sample.csv')",
                    "severity_locations = accident_data[['Temperature(F)','Wind_Chill(F)','Severity','Start_Lng','Start_Lat']]",
                    "gdf_severity = geopandas.GeoDataFrame(severity_locations, geometry=geopandas.points_from_xy(severity_locations.Start_Lng, severity_locations.Start_Lat))",
                    "l = len(gdf_severity)",
                    "gdfs_sample = gdf_severity.sample(int(l/10))",
                    "print(gdfs_sample)"

                ]
            },
            "input_polars":{
                "frame": "gdfs_sample",
                "i": "Severity",
                "req_compile":[
                    "frame"
                ],
                "extra_commands":[
                    "import geopandas",
                    "import geoplot as gplt",
                    "import geoplot.crs as gcrs",
                    "import matplotlib.pyplot as plt",
                    "import pandas as pd",
                    "accident_data = pd.read_csv(r'datasets/US_Accidents_March23/test.csv')",
                    "severity_locations = accident_data[['Temperature(F)','Wind_Chill(F)','Severity','Start_Lng','Start_Lat']]",
                    "gdf_severity = geopandas.GeoDataFrame(severity_locations, geometry=geopandas.points_from_xy(severity_locations.Start_Lng, severity_locations.Start_Lat))",
                    "gdfs_sample = gdf_severity.sample(int(len(gdf_severity)/10))",
                    "print(gdfs_sample)"

                ]
            },
            "input_pyspark_pandas":{
                "frame": "gdfs_sample",
                "i": "Severity",
                "req_compile":[
                    "frame"
                ],
                "extra_commands":[
                    "import geopandas",
                    "import geoplot as gplt",
                    "import geoplot.crs as gcrs",
                    "import matplotlib.pyplot as plt",
                    "import pandas as pd",
                    "accident_data = pd.read_csv(r'datasets/test/sample.csv')",
                    "severity_locations = accident_data[['Temperature(F)','Wind_Chill(F)','Severity','Start_Lng','Start_Lat']]",
                    "gdf_severity = geopandas.GeoDataFrame(severity_locations, geometry=geopandas.points_from_xy(severity_locations.Start_Lng, severity_locations.Start_Lat))",
                    "gdfs_sample = gdf_severity.sample(int(len(gdf_severity)/10))",
                    "print(gdfs_sample)"

                ]
            }
        },
        {
            "method":"plot_geo",
            "input_pandas":{
                "frame": "gdfs_sample",
                "i": "Temperature(F)",
                "req_compile":[
                    "frame"
                ],
                "extra_commands":[
                    "import geopandas",
                    "import geoplot as gplt",
                    "import geoplot.crs as gcrs",
                    "import matplotlib.pyplot as plt",
                    "import pandas as pd",
                    "accident_data = pd.read_csv(r'datasets/test/sample.csv')",
                    "severity_locations = accident_data[['Temperature(F)','Wind_Chill(F)','Severity','Start_Lng','Start_Lat']]",
                    "gdf_severity = geopandas.GeoDataFrame(severity_locations, geometry=geopandas.points_from_xy(severity_locations.Start_Lng, severity_locations.Start_Lat))",
                    "gdfs_sample = gdf_severity.sample(int(len(gdf_severity)/10))",
                    "print(gdfs_sample)"

                ]
            },
            "input_vaex":{
                "frame": "gdfs_sample",
                "i": 0,
                "req_compile":[
                    "frame"
                ],
                "extra_commands":[
                    "import geopandas",
                    "import geoplot as gplt",
                    "import geoplot.crs as gcrs",
                    "import matplotlib.pyplot as plt",
                    "import pandas as pd",
                    "import numpy as np",
                    "import vaex as vx",
                    "accident_data = vx.read_csv(r'datasets/test/sample.csv')",
                    "severity_locations = accident_data[['Temperature(F)','Wind_Chill(F)','Severity','Start_Lng','Start_Lat']]",
                    "start_lng = accident_data['Start_Lng'].to_numpy()",
                    "start_lat = accident_data['Start_Lat'].to_numpy()",
                    "gdf_severity = geopandas.GeoDataFrame(severity_locations, geometry=geopandas.points_from_xy(start_lng, start_lat))",
                    "gdfs_sample = gdf_severity.sample(int(len(gdf_severity)/10))",
                    "print(gdfs_sample)"

                ]
            },
            "input_spark":{
                "frame": "gdfs_sample",
                "i":  "Temperature(F)",
                "req_compile":[
                    "frame"
                ],
                "extra_commands":[
                    "import geopandas",
                    "import geoplot as gplt",
                    "import geoplot.crs as gcrs",
                    "import pandas as pd",
                    "import numpy as np",
                    "from pyspark.sql import DataFrame, SparkSession",
                    "from pyspark.sql.functions import col, expr, lit",
                    "import matplotlib.pyplot as plt",
                    "import pyspark.sql.functions as fn",
                    "accident_data = pd.read_csv(r'datasets/US_Accidents_March23/sample.csv')",
                    "severity_locations = accident_data[['Temperature(F)','Wind_Chill(F)','Severity','Start_Lng','Start_Lat']]",
                    "gdf_severity = geopandas.GeoDataFrame(severity_locations, geometry=geopandas.points_from_xy(severity_locations.Start_Lng, severity_locations.Start_Lat))",
                    "l = len(gdf_severity)",
                    "gdfs_sample = gdf_severity.sample(int(l/10))",
                    "print(gdfs_sample)"

                ]
            },
            "input_polars":{
                "frame": "gdfs_sample",
                "i": "Temperature(F)",
                "req_compile":[
                    "frame"
                ],
                "extra_commands":[
                    "import geopandas",
                    "import geoplot as gplt",
                    "import geoplot.crs as gcrs",
                    "import matplotlib.pyplot as plt",
                    "import pandas as pd",
                    "accident_data = pd.read_csv(r'datasets/US_Accidents_March23/test.csv')",
                    "severity_locations = accident_data[['Temperature(F)','Wind_Chill(F)','Severity','Start_Lng','Start_Lat']]",
                    "gdf_severity = geopandas.GeoDataFrame(severity_locations, geometry=geopandas.points_from_xy(severity_locations.Start_Lng, severity_locations.Start_Lat))",
                    "gdfs_sample = gdf_severity.sample(int(len(gdf_severity)/10))",
                    "print(gdfs_sample)"

                ]
            },
            "input_pyspark_pandas":{
                "frame": "gdfs_sample",
                "i": "Temperature(F)",
                "req_compile":[
                    "frame"
                ],
                "extra_commands":[
                    "import geopandas",
                    "import geoplot as gplt",
                    "import geoplot.crs as gcrs",
                    "import matplotlib.pyplot as plt",
                    "import pandas as pd",
                    "accident_data = pd.read_csv(r'datasets/test/sample.csv')",
                    "severity_locations = accident_data[['Temperature(F)','Wind_Chill(F)','Severity','Start_Lng','Start_Lat']]",
                    "gdf_severity = geopandas.GeoDataFrame(severity_locations, geometry=geopandas.points_from_xy(severity_locations.Start_Lng, severity_locations.Start_Lat))",
                    "gdfs_sample = gdf_severity.sample(int(len(gdf_severity)/10))",
                    "print(gdfs_sample)"

                ]
            }
        },
        {
            "method":"plot_geo",
            "input_pandas":{
                "frame": "gdfs_sample",
                "i": "Wind_Chill(F)",
                "req_compile":[
                    "frame"
                ],
                "extra_commands":[
                    "import geopandas",
                    "import geoplot as gplt",
                    "import geoplot.crs as gcrs",
                    "import matplotlib.pyplot as plt",
                    "import pandas as pd",
                    "accident_data = pd.read_csv(r'datasets/test/sample.csv')",
                    "severity_locations = accident_data[['Temperature(F)','Wind_Chill(F)','Severity','Start_Lng','Start_Lat']]",
                    "gdf_severity = geopandas.GeoDataFrame(severity_locations, geometry=geopandas.points_from_xy(severity_locations.Start_Lng, severity_locations.Start_Lat))",
                    "gdfs_sample = gdf_severity.sample(int(len(gdf_severity)/10))",
                    "print(gdfs_sample)"

                ]
            },
            "input_vaex":{
                "frame": "gdfs_sample",
                "i": 1,
                "req_compile":[
                    "frame"
                ],
                "extra_commands":[
                    "import geopandas",
                    "import geoplot as gplt",
                    "import geoplot.crs as gcrs",
                    "import matplotlib.pyplot as plt",
                    "import pandas as pd",
                    "import numpy as np",
                    "import vaex as vx",
                    "accident_data = vx.read_csv(r'datasets/test/sample.csv')",
                    "severity_locations = accident_data[['Temperature(F)','Wind_Chill(F)','Severity','Start_Lng','Start_Lat']]",
                    "start_lng = accident_data['Start_Lng'].to_numpy()",
                    "start_lat = accident_data['Start_Lat'].to_numpy()",
                    "gdf_severity = geopandas.GeoDataFrame(severity_locations, geometry=geopandas.points_from_xy(start_lng, start_lat))",
                    "gdfs_sample = gdf_severity.sample(int(len(gdf_severity)/10))",
                    "print(gdfs_sample)"

                ]
            },
            "input_spark":{
                "frame": "gdfs_sample",
                "i": "Wind_Chill(F)",
                "req_compile":[
                    "frame"
                ],
                "extra_commands":[
                    "import geopandas",
                    "import geoplot as gplt",
                    "import geoplot.crs as gcrs",
                    "import pandas as pd",
                    "import numpy as np",
                    "from pyspark.sql import DataFrame, SparkSession",
                    "from pyspark.sql.functions import col, expr, lit",
                    "import matplotlib.pyplot as plt",
                    "import pyspark.sql.functions as fn",
                    "accident_data = pd.read_csv(r'datasets/US_Accidents_March23/sample.csv')",
                    "severity_locations = accident_data[['Temperature(F)','Wind_Chill(F)','Severity','Start_Lng','Start_Lat']]",
                    "gdf_severity = geopandas.GeoDataFrame(severity_locations, geometry=geopandas.points_from_xy(severity_locations.Start_Lng, severity_locations.Start_Lat))",
                    "l = len(gdf_severity)",
                    "gdfs_sample = gdf_severity.sample(int(l/10))",
                    "print(gdfs_sample)"

                ]
            },
            "input_polars":{
                "frame": "gdfs_sample",
                "i": "Wind_Chill(F)",
                "req_compile":[
                    "frame"
                ],
                "extra_commands":[
                    "import geopandas",
                    "import geoplot as gplt",
                    "import geoplot.crs as gcrs",
                    "import matplotlib.pyplot as plt",
                    "import pandas as pd",
                    "accident_data = pd.read_csv(r'datasets/US_Accidents_March23/test.csv')",
                    "severity_locations = accident_data[['Temperature(F)','Wind_Chill(F)','Severity','Start_Lng','Start_Lat']]",
                    "gdf_severity = geopandas.GeoDataFrame(severity_locations, geometry=geopandas.points_from_xy(severity_locations.Start_Lng, severity_locations.Start_Lat))",
                    "gdfs_sample = gdf_severity.sample(int(len(gdf_severity)/10))",
                    "print(gdfs_sample)"

                ]
            },
            "input_pyspark_pandas":{
                "frame": "gdfs_sample",
                "i": "Wind_Chill(F)",
                "req_compile":[
                    "frame"
                ],
                "extra_commands":[
                    "import geopandas",
                    "import geoplot as gplt",
                    "import geoplot.crs as gcrs",
                    "import matplotlib.pyplot as plt",
                    "import pandas as pd",
                    "accident_data = pd.read_csv(r'datasets/test/sample.csv')",
                    "severity_locations = accident_data[['Temperature(F)','Wind_Chill(F)','Severity','Start_Lng','Start_Lat']]",
                    "gdf_severity = geopandas.GeoDataFrame(severity_locations, geometry=geopandas.points_from_xy(severity_locations.Start_Lng, severity_locations.Start_Lat))",
                    "gdfs_sample = gdf_severity.sample(int(len(gdf_severity)/10))",
                    "print(gdfs_sample)"

                ]
            }
        },
        {
            "method": "force_execution",
            "input": {}
        }



    ],

    "data_transformation": [
        {
            "method":"change_date_time_format",
            "input_pandas":{
                "column":"Start_Time",
                "format":"%Y-%m-%d %H:%M:%S"
            },
            "input_vaex":{
                "column":"Start_Time",
                "format":"%Y-%m-%d %H:%M:%S"    
            },
            "input_spark":{
                "column":"Start_Time",
                "format":"yyyy-MM-dd HH:mm:ss"    
            },
            "input_polars": {
                "column": "Start_Time",
                "format": "%Y-%m-%d %H:%M:%S"
            },
            "input_pyspark_pandas": {
                "column": "Start_Time",
                "format": "%Y-%m-%d %H:%M:%S"
            }
        },
        {
            "method":"change_date_time_format",
            "input_pandas":{
                "column":"End_Time",
                "format":"%Y-%m-%d %H:%M:%S"
            },
            "input_vaex":{
                "column":"End_Time",
                "format":"%Y-%m-%d %H:%M:%S"    
            },
            "input_spark":{
                "column":"End_Time",
                "format":"yyyy-MM-dd HH:mm:ss"
            },
            "input_polars": {
                "column": "End_Time",
                "format": "%Y-%m-%d %H:%M:%S"
            },
            "input_pyspark_pandas": {
                "column": "End_Time",
                "format": "%Y-%m-%d %H:%M:%S"
            }
        },
        {
            "method":"change_date_time_format",
            "input_pandas":{
                "column":"Weather_Timestamp",
                "format":"%Y-%m-%d %H:%M:%S"
            },
            "input_vaex":{
                "column":"Weather_Timestamp",
                "format":"%Y-%m-%d %H:%M:%S"    
            },
            "input_spark":{
                "column":"Weather_Timestamp",
                "format":"yyyy-MM-dd HH:mm:ss"
            },
            "input_polars": {
                "column": "Weather_Timestamp",
                "format": "%Y-%m-%d %H:%M:%S"
            },
            "input_pyspark_pandas": {
                "column": "Weather_Timestamp",
                "format": "%Y-%m-%d %H:%M:%S"
            }
        },
        {
            "method":"split",
            "input_pandas":{
                "column":"Weather_Timestamp",
                "sep": " ",
                "splits": 2,
                "col_names": ["day", "hour"]

            },
            "input_vaex":{
                "column":"Weather_Timestamp",
                "sep": " ",
                "splits": 2,
                "col_names": ["day", "hour"]

            },
            "input_spark":{
                "column":"Weather_Timestamp",
                "sep": " ",
                "splits": 2,
                "col_names": ["day", "hour"]
            },
            "input_polars":{
                "column":"Weather_Timestamp",
                "sep": " ",
                "splits": 2,
                "col_names": ["day", "hour"]
            },
            "input_pyspark_pandas":{
                "column":"Weather_Timestamp",
                "sep": " ",
                "splits": 2,
                "col_names": ["day", "hour"]

            }
        },
        {
            "method":"delete_columns",
            "input_pandas":{
                "columns":["ID","Start_Time","End_Time", "Weather_Timestamp"]
            },
            "input_vaex":{
                "columns":["ID","Start_Time","End_Time", "Weather_Timestamp"]
            },
            "input_spark":{
                "columns":["ID","Start_Time","End_Time", "Weather_Timestamp"]
            },
            "input_polars":{
                "columns": ["ID","Start_Time","End_Time", "Weather_Timestamp"]
            },
            "input_pyspark_pandas":{
                "columns": ["ID","Start_Time","End_Time", "Weather_Timestamp"]
            }
        },
        {
            "method":"calc_column",
            "input_pandas":{
                "col_name":"abs_end_lng_change",
                "columns":["End_Lng", "Start_Lng"],
                "f":"lambda row: abs(row['End_Lng'] - row['Start_Lng'])"
            },
            "input_vaex":{
                "col_name":"abs_end_lng_change",
                "columns":["End_Lng", "Start_Lng"],
                "f":"lambda x, y: abs(x - y)"
            },
            "input_spark":{
                "col_name":"abs_end_lng_change",
                "columns":["End_Lng", "Start_Lng"],
                "f":"lambda x, y: abs(x - y)"
            },
            "input_polars":{
                "col_name":"abs_end_lng_change",
                "columns":["End_Lng"],
                "f":"lambda x: abs(x)"
            },
            "input_pyspark_pandas":{
                "col_name":"abs_end_lng_change",
                "columns":["End_Lng", "Start_Lng"],
                "f":"(df['End_Lng'] - df['Start_Lng']).abs()"
            }
        },
        {
            "method":"calc_column",
            "input_pandas":{
                "col_name":"abs_end_lat_change",
                "columns":["End_Lat", "Start_Lat"],
                "f":"lambda row: abs(row['End_Lat'] - row['Start_Lat'])"
            },
            "input_vaex":{
                "col_name":"abs_end_lat_change",
                "columns":["End_Lat", "Start_Lat"],
                "f":"lambda x, y: abs(x - y)"
            },
            "input_spark":{
                "col_name":"abs_end_lat_change",
                "columns":["End_Lat", "Start_Lat"],
                "f":"lambda x, y: abs(x - y)"
            },
            "input_polars":{
                "col_name":"abs_end_lat_change",
                "columns":["End_Lat"],
                "f":"lambda x: abs(x)"
            },
            "input_pyspark_pandas":{
                "col_name":"abs_end_lat_change",
                "columns":["End_Lat", "Start_Lat"],
                "f":"(df['End_Lat'] - df['Start_Lat']).abs()"
            }
        },
        {
            "method":"simple_imputer",
            "input_pandas":{
                "columns":["Temperature(F)","Wind_Chill(F)","Humidity(%)","Pressure(in)","Visibility(mi)","Wind_Speed(mph)","Precipitation(in)"]
            },
            "input_vaex":{
                "columns":["Temperature(F)","Wind_Chill(F)","Humidity(%)","Pressure(in)","Visibility(mi)","Wind_Speed(mph)","Precipitation(in)"]    
            },
            "input_spark":{
                "columns":["Temperature(F)","Wind_Chill(F)","Humidity(%)","Pressure(in)","Visibility(mi)","Wind_Speed(mph)","Precipitation(in)"]
            },
            "input_polars":{
                "columns":["Temperature(F)","Wind_Chill(F)","Humidity(%)","Pressure(in)","Visibility(mi)","Wind_Speed(mph)","Precipitation(in)"]
            },
            "input_pyspark_pandas":{
                "columns":["Temperature(F)","Wind_Chill(F)","Humidity(%)","Pressure(in)","Visibility(mi)","Wind_Speed(mph)","Precipitation(in)"]
            }
        },
        {
            "method":"pca",
            "input_pandas":{
                "data_pca":"data_pca",
                "n_dim": 3,
                "req_compile":[
                    "data_pca"
                ],
                "extra_commands":[
                    "import pandas as pd",
                    "accident_data = pd.read_csv(r'datasets/test/sample.csv')",
                    "numerical = ['Start_Lat', 'Start_Lng', 'End_Lat', 'End_Lng', 'Distance(mi)', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)','Wind_Speed(mph)', 'Precipitation(in)']",
                    "accident_sample = accident_data[numerical].sample(int(len(accident_data)/100))",
                    "data_pca = (accident_sample - accident_sample.mean(axis=0)) / accident_sample.std(axis=0)"
                ]
            },
            "input_vaex":{
                "data_pca":"data_pca",
                "req_compile":[
                    "data_pca"
                ],
                "extra_commands":[
                    "import vaex as vx",
                    "accident_data = vx.read_csv(r'datasets/test/sample.csv')",
                    "numerical = ['Start_Lat', 'Start_Lng', 'End_Lat', 'End_Lng', 'Distance(mi)', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)','Wind_Speed(mph)', 'Precipitation(in)']",
                    "accident_sample = accident_data[numerical].sample(int(len(accident_data)/100))",
                    "data_pca = accident_sample.dropna()"
                    
                ]
            },
            "input_spark":{
                "data_pca":"data_pca",
                "n_dim": 3,
                "req_compile":[
                    "data_pca"
                ],
                "extra_commands":[
                    "import pandas as pd",
                    "accident_data = pd.read_csv(r'datasets/US_Accidents_March23/sample.csv')",
                    "numerical = ['Start_Lat', 'Start_Lng', 'End_Lat', 'End_Lng', 'Distance(mi)', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)','Wind_Speed(mph)', 'Precipitation(in)']",
                    "accident_sample = accident_data[numerical].sample(int(len(accident_data)/100))",
                    "data_pca = (accident_sample - accident_sample.mean(axis=0)) / accident_sample.std(axis=0)"
                ]
            },
            "input_polars":{
                "data_pca":"data_pca",
                "n_dim": 3,
                "req_compile":[
                    "data_pca"
                ],
                "extra_commands":[
                    "import polars as pl",
                    "accident_data = pl.read_csv(r'datasets/test/sample.csv')",
                    "numerical = ['Start_Lat', 'Start_Lng', 'End_Lat', 'End_Lng', 'Distance(mi)', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)','Wind_Speed(mph)', 'Precipitation(in)']",
                    "accident_sample = accident_data[numerical].sample(int(len(accident_data)/100))",
                    "data_pca = accident_sample.select([((pl.col(column) - accident_sample.select(pl.col(column).mean()).get_column(column).to_numpy()[0]) / accident_sample.select(pl.col(column).std()).get_column(column).to_numpy()[0]).alias(column) for column in accident_sample.columns])"
                ]
            },
            "input_pyspark_pandas":{
                "data_pca":"data_pca",
                "n_dim": 3,
                "req_compile":[
                    "data_pca"
                ],
                "extra_commands":[
                    "import pandas as pd",
                    "accident_data = pd.read_csv(r'datasets/test/sample.csv')",
                    "numerical = ['Start_Lat', 'Start_Lng', 'End_Lat', 'End_Lng', 'Distance(mi)', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)','Wind_Speed(mph)', 'Precipitation(in)']",
                    "accident_sample = accident_data[numerical].sample(frac= len(accident_data)/100 / len(accident_data))",
                    "data_pca = accident_sample.dropna()"
                ]
            }
        },
        {
            "method": "force_execution",
            "input": {}    
        }

    ],

    "data_cleaning": [
        {
            "method":"replace",
            "input_pandas":{
                "columns":["Weather_Condition"],
                "to_replace": {
                    "Thunder": "Thunderstorm"
                }
            },
            "input_vaex":{
                "columns":["Weather_Condition"],
                "to_replace": {
                    "Thunder": "Thunderstorm"
                }
            },
            "input_spark":{
                "columns":["Weather_Condition"],
                "value": "Thunderstorm",
                "regex": "False",
                "to_replace": "Thunder"
            },
            "input_polars":{
                "columns":["Weather_Condition"],
                "value": "Thunderstorm",
                "regex": "False",
                "to_replace": "Thunder"
            },
            "input_pyspark_pandas":{
                "columns":["Weather_Condition"],
                "value": "Thunderstorm",
                "regex": "False",
                "to_replace": "Thunder"
            }
        },
        {
            "method":"replace",
            "input_pandas":{
                "columns":["Weather_Condition"],
                "to_replace": {
                    "T-Storm": "Thunderstorm"
                }
            },
            "input_vaex":{
                "columns":["Weather_Condition"],
                "to_replace": {
                    "T-Storm": "Thunderstorm"
                }
            },
            "input_spark":{
                "columns":["Weather_Condition"],
                "value": "Thunderstorm",
                "regex": "False",
                "to_replace": "T-Storm"
            },
            "input_polars":{
                "columns":["Weather_Condition"],
                "value": "Thunderstorm",
                "regex": "False",
                "to_replace": "T-Storm"
            },
            "input_pyspark_pandas":{
                "columns":["Weather_Condition"],
                "value": "Thunderstorm",
                "regex": "False",
                "to_replace": "T-Storm"
            }
        },
        {
            "method":"replace",
            "input_pandas":{
                "columns":["Weather_Condition"],
                "to_replace": {
                    "T-Storm / Windy": "Thunderstorm / Windy"
                }
            },
            "input_vaex":{
                "columns":["Weather_Condition"],
                "to_replace": {
                    "T-Storm / Windy": "Thunderstorm / Windy"
                }
            },
            "input_spark":{
                "columns":["Weather_Condition"],
                "value": "Thunderstorm / Windy",
                "regex": "False",
                "to_replace": "T-Storm / Windy"
            },
            "input_polars":{
                "columns":["Weather_Condition"],
                "value": "Thunderstorm / Windy",
                "regex": "False",
                "to_replace": "T-Storm / Windy"
            },
            "input_pyspark_pandas":{
                "columns":["Weather_Condition"],
                "value": "Thunderstorm / Windy",
                "regex": "False",
                "to_replace": "T-Storm / Windy"
            }
        },
        {
            "method":"replace",
            "input_pandas":{
                "columns":["Weather_Condition"],
                "to_replace": {
                    "Thunder / Windy": "Thunderstorm / Windy"
                }
            },
            "input_vaex":{
                "columns":["Weather_Condition"],
                "to_replace": {
                    "Thunder / Windy": "Thunderstorm / Windy"
                }
            },
            "input_spark":{
                "columns":["Weather_Condition"],
                "value": "Thunderstorm / Windy",
                "regex": "False",
                "to_replace": "Thunder / Windy"
            },
            "input_polars":{
                "columns":["Weather_Condition"],
                "value": "Thunderstorm / Windy",
                "regex": "False",
                "to_replace": "Thunder / Windy"
            },
            "input_pyspark_pandas":{
                "columns":["Weather_Condition"],
                "value": "Thunderstorm / Windy",
                "regex": "False",
                "to_replace": "Thunder / Windy"
            }
        },
        {
            "method":"replace",
            "input_pandas":{
                "columns":["Weather_Condition"],
                "to_replace": {
                    "Heavy Rain Shower": "Heavy Rain"
                }
            },
            "input_vaex":{
                "columns":["Weather_Condition"],
                "to_replace": {
                    "Heavy Rain Shower": "Heavy Rain"
                }
            },
            "input_spark":{
                "columns":["Weather_Condition"],
                "value": "Heavy Rain",
                "regex": "False",
                "to_replace": "Heavy Rain Shower"
            },
            "input_polars":{
                "columns":["Weather_Condition"],
                "value": "Heavy Rain",
                "regex": "False",
                "to_replace": "Heavy Rain Shower"
            },
            "input_pyspark_pandas":{
                "columns":["Weather_Condition"],
                "value": "Heavy Rain",
                "regex": "False",
                "to_replace": "Heavy Rain Shower"
            }
        },
        {
            "method":"replace",
            "input_pandas":{
                "columns":["Weather_Condition"],
                "to_replace": {
                    "Heavy Rain Showers": "Heavy Rain"
                }
            },
            "input_vaex":{
                "columns":["Weather_Condition"],
                "to_replace": {
                    "Heavy Rain Showers": "Heavy Rain"
                }
            },
            "input_spark":{
                "columns":["Weather_Condition"],
                "value": "Heavy Rain",
                "regex": "False",
                "to_replace": "Heavy Rain Showers"
            },
            "input_polars":{
                "columns":["Weather_Condition"],
                "value": "Heavy Rain",
                "regex": "False",
                "to_replace": "Heavy Rain Showers"
            },
            "input_pyspark_pandas":{
                "columns":["Weather_Condition"],
                "value": "Heavy Rain",
                "regex": "False",
                "to_replace": "Heavy Rain Showers"
            }
        },
        {
            "method":"replace",
            "input_pandas":{
                "columns":["Weather_Condition"],
                "to_replace": {
                    "Light Rain Shower": "Light Rain"
                }
            },
            "input_vaex":{
                "columns":["Weather_Condition"],
                "to_replace": {
                    "Light Rain Shower": "Light Rain"
                }
            },
            "input_spark":{
                "columns":["Weather_Condition"],
                "value": "Light Rain",
                "regex": "False",
                "to_replace": "Light Rain Shower"
            },
            "input_polars":{
                "columns":["Weather_Condition"],
                "value": "Light Rain",
                "regex": "False",
                "to_replace": "Light Rain Shower"
            },
            "input_pyspark_pandas":{
                "columns":["Weather_Condition"],
                "value": "Light Rain",
                "regex": "False",
                "to_replace": "Light Rain Shower"
            }
        },
        {
            "method":"replace",
            "input_pandas":{
                "columns":["Weather_Condition"],
                "to_replace": {
                    "Light Rain Showers": "Light Rain"
                }
            },
            "input_vaex":{
                "columns":["Weather_Condition"],
                "to_replace": {
                    "Light Rain Showers": "Light Rain"
                }
            },
            "input_spark":{
                "columns":["Weather_Condition"],
                "value": "Light Rain",
                "regex": "False",
                "to_replace": "Light Rain Showers"
            },
            "input_polars":{
                "columns":["Weather_Condition"],
                "value": "Light Rain",
                "regex": "False",
                "to_replace": "Light Rain Showers"
            },
            "input_pyspark_pandas":{
                "columns":["Weather_Condition"],
                "value": "Light Rain",
                "regex": "False",
                "to_replace": "Light Rain Showers"
            }
        },
        {
            "method":"replace",
            "input_pandas":{
                "columns":["Weather_Condition"],
                "to_replace": {
                    "Rain Shower": "Rain"
                }
            },
            "input_vaex":{
                "columns":["Weather_Condition"],
                "to_replace": {
                    "Rain Shower": "Rain"
                }
            },
            "input_spark":{
                "columns":["Weather_Condition"],
                "value": "Rain",
                "regex": "False",
                "to_replace": "Rain Shower"
            },
            "input_polars":{
               "columns":["Weather_Condition"],
                "value": "Rain",
                "regex": "False",
                "to_replace": "Rain Shower"
            },
            "input_pyspark_pandas":{
               "columns":["Weather_Condition"],
                "value": "Rain",
                "regex": "False",
                "to_replace": "Rain Shower"
            }
        },
        {
            "method":"replace",
            "input_pandas":{
                "columns":["Weather_Condition"],
                "to_replace": {
                    "Rain Showers": "Rain"
                }
            },
            "input_vaex":{
                "columns":["Weather_Condition"],
                "to_replace": {
                    "Rain Showers": "Rain"
                }
            },
            "input_spark":{
                "columns":["Weather_Condition"],
                "value": "Rain",
                "regex": "False",
                "to_replace": "Rain Showers"
            },
            "input_polars":{
                "columns":["Weather_Condition"],
                "value": "Rain",
                "regex": "False",
                "to_replace": "Rain Showers"
            },
            "input_pyspark_pandas":{
                "columns":["Weather_Condition"],
                "value": "Rain",
                "regex": "False",
                "to_replace": "Rain Showers"
            }
        },
        {
            "method":"replace",
            "input_pandas":{
                "columns":["Wind_Direction"],
                "to_replace": {
                    "West": "W"
                }
            },
            "input_vaex":{
                "columns":["Wind_Direction"],
                "to_replace": {
                    "West": "W"
                }
            },
            "input_spark":{
                "columns":["Wind_Direction"],
                "value": "W",
                "regex": "False",
                "to_replace": "West"
            },
            "input_polars":{
                "columns":["Wind_Direction"],
                "value": "W",
                "regex": "False",
                "to_replace": "West"
            },
            "input_pyspark_pandas":{
                "columns":["Wind_Direction"],
                "value": "W",
                "regex": "False",
                "to_replace": "West"
            }
        },
        {
            "method":"replace",
            "input_pandas":{
                "columns":["Wind_Direction"],
                "to_replace": {
                    "Variable": "VAR"
                }
            },
            "input_vaex":{
                "columns":["Wind_Direction"],
                "to_replace": {
                    "Variable": "VAR"
                }
            },
            "input_spark":{
                "columns":["Wind_Direction"],
                "value": "VAR",
                "regex": "False",
                "to_replace": "Variable"
            },
            "input_polars":{
                "columns":["Wind_Direction"],
                "value": "VAR",
                "regex": "False",
                "to_replace": "Variable"
            },
            "input_pyspark_pandas":{
                "columns":["Wind_Direction"],
                "value": "VAR",
                "regex": "False",
                "to_replace": "Variable"
            }
        },
        {
            "method":"replace",
            "input_pandas":{
                "columns":["Wind_Direction"],
                "to_replace": {
                    "South": "S"
                }
            },
            "input_vaex":{
                "columns":["Wind_Direction"],
                "to_replace": {
                    "South": "S"
                }
            },
            "input_spark":{
                "columns":["Wind_Direction"],
                "value": "S",
                "regex": "False",
                "to_replace": "South"
            },
            "input_polars":{
                "columns":["Wind_Direction"],
                "value": "S",
                "regex": "False",
                "to_replace": "South"
            },
            "input_pyspark_pandas":{
                "columns":["Wind_Direction"],
                "value": "S",
                "regex": "False",
                "to_replace": "South"
            }
        },
        {
            "method":"replace",
            "input_pandas":{
                "columns":["Wind_Direction"],
                "to_replace": {
                    "Calm": "CALM"
                }
            },
            "input_vaex":{
                "columns":["Wind_Direction"],
                "to_replace": {
                    "Calm": "CALM"
                }
            },
            "input_spark":{
                "columns":["Wind_Direction"],
                "value": "CALM",
                "regex": "False",
                "to_replace": "Calm"
            },
            "input_polars":{
                "columns":["Wind_Direction"],
                "value": "CALM",
                "regex": "False",
                "to_replace": "Calm"
            },
            "input_pyspark_pandas":{
                "columns":["Wind_Direction"],
                "value": "CALM",
                "regex": "False",
                "to_replace": "Calm"
            }
        },
        {
            "method":"replace",
            "input_pandas":{
                "columns":["Wind_Direction"],
                "to_replace": {
                    "East": "E"
                }
            },
            "input_vaex":{
                "columns":["Wind_Direction"],
                "to_replace": {
                    "East": "E"
                }
            },
            "input_spark":{
                "columns":["Wind_Direction"],
                "value": "E",
                "regex": "False",
                "to_replace": "East"
            },
            "input_polars":{
                "columns":["Wind_Direction"],
                "value": "E",
                "regex": "False",
                "to_replace": "East"
            },
            "input_pyspark_pandas":{
                "columns":["Wind_Direction"],
                "value": "E",
                "regex": "False",
                "to_replace": "East"
            }
        },
        {
            "method":"replace",
            "input_pandas":{
                "columns":["Wind_Direction"],
                "to_replace": {
                    "North": "N"
                }
            },
            "input_vaex":{
                "columns":["Wind_Direction"],
                "to_replace": {
                    "North": "N"
                }
            },
            "input_spark":{
                "columns":["Wind_Direction"],
                "value": "N",
                "regex": "False",
                "to_replace": "North"
            },
            "input_polars":{
                "columns":["Wind_Direction"],
                "value": "N",
                "regex": "False",
                "to_replace": "North"
            },
            "input_pyspark_pandas":{
                "columns":["Wind_Direction"],
                "value": "N",
                "regex": "False",
                "to_replace": "North"
            }
        },
        {
            "method":"fill_nan",
            "input_pandas":{
                "value":"KBTR",
                "columns":["Airport_Code"]
            },
            "input_vaex":{
                "value":"KBTR",
                "columns":["Airport_Code"]    
            },
            "input_spark":{
                "value":"KBTR",
                "columns":["Airport_Code"]
            },
            "input_polars":{
                "value":"KBTR",
                "columns":["Airport_Code"]
            },
            "input_pyspark_pandas":{
                "value":"KBTR",
                "columns":["Airport_Code"]
            }
        },
        {
            "method":"fill_nan",
            "input_pandas":{
                "value":"CALM",
                "columns":["Wind_Direction"]
            },
            "input_vaex":{
                "value":"CALM",
                "columns":["Wind_Direction"]    
            },
            "input_spark":{
                "value":"CALM",
                "columns":["Wind_Direction"]
            },
            "input_polars":{
                "value":"CALM",
                "columns":["Wind_Direction"]
            },
            "input_pyspark_pandas":{
                "value":"CALM",
                "columns":["Wind_Direction"]
            }
        },
        {
            "method":"fill_nan",
            "input_pandas":{
                "value":"Day",
                "columns":["Sunrise_Sunset"]
            },
            "input_vaex":{
                "value":"Day",
                "columns":["Sunrise_Sunset"]    
            },
            "input_spark":{
                "value":"Day",
                "columns":["Sunrise_Sunset"]
            },
            "input_polars":{
                "value":"Day",
                "columns":["Sunrise_Sunset"]
            },
            "input_pyspark_pandas":{
                "value":"Day",
                "columns":["Sunrise_Sunset"]
            }
        },
        {
            "method":"delete_empty_rows",
            "input_pandas":{
                "columns":"all"
            },
            "input_vaex":{
                "columns":"all"
            },
            "input_spark":{
                "columns":"all"
            },
            "input_polars":{
                "columns":"all"
            },
            "input_pyspark_pandas":{
                "columns":"all"
            }
        },
        {
            "method":"split",
            "input_pandas":{
                "column":"Zipcode",
                "sep": "-",
                "splits": 2,
                "col_names": ["Zipcode_ok", "Zipcode_not_ok"]

            },
            "input_vaex":{
                "column":"Zipcode",
                "sep": "-",
                "splits": 2,
                "col_names": ["Zipcode_ok", "Zipcode_not_ok"]
            },
            "input_spark":{
                "column":"Zipcode",
                "sep": "-",
                "splits": 2,
                "col_names": ["Zipcode_ok", "Zipcode_not_ok"]
            },
            "input_polars":{
                "column":"Zipcode",
                "sep": "-",
                "splits": 2,
                "col_names": ["Zipcode_ok", "Zipcode_not_ok"]
            },
            "input_pyspark_pandas":{
                "column":"Zipcode",
                "sep": "-",
                "splits": 2,
                "col_names": ["Zipcode_ok", "Zipcode_not_ok"]
            }
        },
        {
            "method": "force_execution",
            "input": {}
        
        }

    ],

    "output": [



    ]


}